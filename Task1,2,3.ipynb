{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA5cAMDT7vkg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load datasets\n",
        "customers = pd.read_csv(\"Customers.csv\")\n",
        "products = pd.read_csv(\"Products.csv\")\n",
        "transactions = pd.read_csv(\"Transactions.csv\")\n",
        "\n",
        "# Task 1: Exploratory Data Analysis (EDA)\n",
        "def perform_eda():\n",
        "    print(\"Customers Dataset Overview:\")\n",
        "    print(customers.info())\n",
        "    print(customers.describe())\n",
        "\n",
        "    print(\"\\nProducts Dataset Overview:\")\n",
        "    print(products.info())\n",
        "    print(products.describe())\n",
        "\n",
        "    print(\"\\nTransactions Dataset Overview:\")\n",
        "    print(transactions.info())\n",
        "    print(transactions.describe())\n",
        "\n",
        "    # Example Insights\n",
        "    print(\"\\nTop 5 Regions by Customer Count:\")\n",
        "    print(customers['Region'].value_counts().head())\n",
        "\n",
        "    print(\"\\nTop 5 Product Categories by Sales:\")\n",
        "    sales_per_category = transactions.merge(products, on='ProductID')\n",
        "    sales_per_category = sales_per_category.groupby('Category')['TotalValue'].sum().sort_values(ascending=False)\n",
        "    print(sales_per_category.head())\n",
        "\n",
        "# Task 2: Lookalike Model\n",
        "def build_lookalike_model():\n",
        "    # Create feature vectors for customers\n",
        "    customer_features = transactions.groupby('CustomerID')[['Quantity', 'TotalValue']].sum()\n",
        "    customer_features = customer_features.merge(customers[['CustomerID', 'Region']], on='CustomerID', how='left')\n",
        "\n",
        "    # Encode regions as one-hot\n",
        "    customer_features = pd.get_dummies(customer_features, columns=['Region'], drop_first=True)\n",
        "\n",
        "    # Normalize data\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(customer_features.drop('CustomerID', axis=1))\n",
        "\n",
        "    # Compute similarity\n",
        "    similarity_matrix = cosine_similarity(normalized_features)\n",
        "\n",
        "    # Find top 3 similar customers for each customer\n",
        "    similarity_df = pd.DataFrame(similarity_matrix, index=customer_features['CustomerID'], columns=customer_features['CustomerID'])\n",
        "\n",
        "    lookalike_results = {}\n",
        "    for customer_id in similarity_df.index:\n",
        "        similar_customers = similarity_df.loc[customer_id].sort_values(ascending=False).iloc[1:4]\n",
        "        lookalike_results[customer_id] = list(zip(similar_customers.index, similar_customers.values))\n",
        "\n",
        "    return lookalike_results\n",
        "\n",
        "# Task 3: Customer Segmentation\n",
        "def perform_clustering():\n",
        "    # Prepare data\n",
        "    customer_data = transactions.groupby('CustomerID')[['Quantity', 'TotalValue']].sum()\n",
        "    customer_data = customer_data.merge(customers[['CustomerID', 'Region']], on='CustomerID', how='left')\n",
        "    customer_data = pd.get_dummies(customer_data, columns=['Region'], drop_first=True)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(customer_data.drop('CustomerID', axis=1))\n",
        "\n",
        "    # Apply KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "    clusters = kmeans.fit_predict(scaled_data)\n",
        "    customer_data['Cluster'] = clusters\n",
        "\n",
        "    # Davies-Bouldin Index\n",
        "    from sklearn.metrics import davies_bouldin_score\n",
        "    db_index = davies_bouldin_score(scaled_data, clusters)\n",
        "    print(f\"Davies-Bouldin Index: {db_index}\")\n",
        "\n",
        "    # Visualize clusters\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_data = pca.fit_transform(scaled_data)\n",
        "    customer_data['PCA1'] = pca_data[:, 0]\n",
        "    customer_data['PCA2'] = pca_data[:, 1]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(data=customer_data, x='PCA1', y='PCA2', hue='Cluster', palette='viridis')\n",
        "    plt.title(\"Customer Segments\")\n",
        "    plt.show()\n",
        "\n",
        "# Execute Tasks\n",
        "perform_eda()\n",
        "lookalike_results = build_lookalike_model()\n",
        "perform_clustering()\n",
        "\n",
        "# Save Lookalike Results\n",
        "lookalike_df = pd.DataFrame.from_dict(lookalike_results, orient='index', columns=['Top1', 'Top2', 'Top3'])\n",
        "lookalike_df.to_csv(\"Lookalike.csv\", index_label='CustomerID')\n"
      ]
    }
  ]
}